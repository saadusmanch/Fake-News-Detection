{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd \"D:\\University\\FIT3162\\Project\\Fake-News-Detection\\Feature Extraction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sentiment_vader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtraction:\n",
    "    \"\"\"\n",
    "    This class is used to build a pipeline for sentiment feature extraction using Vader\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.post_file = \"cleaned_df.csv\"\n",
    "        self.comment_file = \"cleaned_comments.csv\"\n",
    "        self.post_df = None\n",
    "        self.comment_df = None\n",
    "        self.comment_scores = {}\n",
    "    \n",
    "    def read_datasets(self):\n",
    "        self.post_df = pd.read_csv(self.post_file, index_col = 0)\n",
    "        self.comment_df = pd.read_csv(self.comment_file, index_col=0)\n",
    "    \n",
    "    def print_statistics(self):\n",
    "        print(\"Number of Posts\", len(self.post_df))\n",
    "        print(\"Number of Comments\", len(self.comment_df))\n",
    "        print(\"Number of Fake Posts\", len(self.post_df.loc[self.post_df['2_way_label'] == 0]))\n",
    "        print(\"Number of True Posts\", len(self.post_df.loc[self.post_df['2_way_label'] == 1]))\n",
    "        \n",
    "    def get_sample_posts(self, sample_size):\n",
    "        self.post_df = self.post_df.sample(sample_size, random_state = 1).reset_index(drop=True)\n",
    "        \n",
    "    def filter_comments(self):\n",
    "        ids = self.post_df.id.unique()\n",
    "        self.comment_df = self.comment_df[self.comment_df['submission_id'].isin(ids)]\n",
    "        self.comment_df = self.comment_df.reset_index(drop=True)\n",
    "        \n",
    "    def build_comment_score(self):\n",
    "        #creating hashtables with post id as key\n",
    "        for ind in self.post_df.index:\n",
    "            # hastable for score of comments\n",
    "            self.comment_scores[self.post_df['id'][ind]] = [0, 0]\n",
    "        sentiment_vader.build_comment_dictionary(self.comment_df, self.comment_scores)\n",
    "        \n",
    "    def cmnt_sentiment_column(self):\n",
    "        \"\"\"\n",
    "        Add the comment sentiment Column to Post dataset\n",
    "        \"\"\"\n",
    "        temp = list(self.comment_scores.values())\n",
    "        score = [x[0]/x[1] if x[1] > 0 else x[1] for x in temp]\n",
    "        num_comments = [x[1] for x in temp]\n",
    "        \n",
    "        self.post_df[\"num_comments\"] = num_comments\n",
    "        self.post_df[\"comment_sentiment\"] = score\n",
    "            \n",
    "    def post_sentiment_column(self):\n",
    "        self.post_df['post_sentiment'] = self.post_df.apply(lambda x: sentiment_vader.post_sentiment(x['title']), axis=1)\n",
    "        \n",
    "    def build_pipeline(self):\n",
    "        print(\"Step 1: Reading Dataset\")\n",
    "        self.read_datasets()\n",
    "        print(\"Step 2: Filter Posts\")\n",
    "        self.get_sample_posts(6)\n",
    "        print(\"Step 3: Filter Comments\")\n",
    "        self.filter_comments()\n",
    "        print(\"Step 4: Building Comment Score Dictionary\")\n",
    "        self.build_comment_score()\n",
    "        print(\"Step 6: Add Comment Score Column\")\n",
    "        self.cmnt_sentiment_column()\n",
    "        print(\"Step 7: Add Post Score Column\")\n",
    "        self.post_sentiment_column()\n",
    "        print(\"---DONE---\")\n",
    "        \n",
    "    def get_post_dataset(self):\n",
    "        return self.post_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Reading Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:569: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Filter Posts\n",
      "Step 3: Filter Comments\n",
      "Step 4: Building Comment Score Dictionary\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'regex._regex'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9c4ed1687c59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfeature_extraction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeatureExtraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Read the datasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-7184d6426686>\u001b[0m in \u001b[0;36mbuild_pipeline\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_comments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Step 4: Building Comment Score Dictionary\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_comment_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Step 6: Add Comment Score Column\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmnt_sentiment_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-7184d6426686>\u001b[0m in \u001b[0;36mbuild_comment_score\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;31m# hastable for score of comments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomment_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0msentiment_vader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_comment_dictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomment_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomment_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcmnt_sentiment_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\University\\FIT3162\\Project\\Fake-News-Detection\\Feature Extraction\\sentiment_vader.py\u001b[0m in \u001b[0;36mbuild_comment_dictionary\u001b[1;34m(df, comment_scores)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Building Sentiment Analyser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mvaderSentiment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvaderSentiment\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0manalyser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrammar\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobability\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\text.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mf_measure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBigramAssocMeasures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollocations\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBigramCollocationFinder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m ConcordanceLine = namedtuple(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcasual\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTweetTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasual_tokenize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmwe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMWETokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestructive\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNLTKWordTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\nltk\\tokenize\\casual.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m######################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mregex\u001b[0m  \u001b[1;31m# https://github.com/nltk/nltk/issues/2409\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhtml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\regex\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mregex\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0m__all__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__all__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\regex\\regex.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;31m# Internals.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_regex_core\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_regex_core\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    404\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_regex\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_regex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mthreading\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRLock\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_RLock\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\regex\\_regex_core.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_regex\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_regex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m __all__ = [\"A\", \"ASCII\", \"B\", \"BESTMATCH\", \"D\", \"DEBUG\", \"E\", \"ENHANCEMATCH\",\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'regex._regex'"
     ]
    }
   ],
   "source": [
    "# Create a pipeline object\n",
    "feature_extraction = FeatureExtraction()\n",
    "# Read the datasets\n",
    "feature_extraction.build_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Posts 10000\n",
      "Number of Comments 820028\n",
      "Number of Fake Posts 5194\n",
      "Number of True Posts 4806\n"
     ]
    }
   ],
   "source": [
    "# Print Statistics\n",
    "feature_extraction.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df = feature_extraction.get_post_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>domain</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>2_way_label</th>\n",
       "      <th>comment_sentiment</th>\n",
       "      <th>post_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>can i still cash in my brain needs to rest</td>\n",
       "      <td>2017-06-17 12:58:22</td>\n",
       "      <td>self.SubredditSimulator</td>\n",
       "      <td>6hrnqx</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>subredditsimulator</td>\n",
       "      <td>Can I still cash in my brain needs to rest</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.034000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how did the wheat say to its son photoshop</td>\n",
       "      <td>2019-07-27 18:58:22</td>\n",
       "      <td>self.SubredditSimulator</td>\n",
       "      <td>cig6ey</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>subredditsimulator</td>\n",
       "      <td>How did the wheat say to its son Photoshop?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.196715</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  clean_title          created_utc  \\\n",
       "0  can i still cash in my brain needs to rest  2017-06-17 12:58:22   \n",
       "1  how did the wheat say to its son photoshop  2019-07-27 18:58:22   \n",
       "\n",
       "                    domain      id  num_comments  score           subreddit  \\\n",
       "0  self.SubredditSimulator  6hrnqx            20     17  subredditsimulator   \n",
       "1  self.SubredditSimulator  cig6ey            20      3  subredditsimulator   \n",
       "\n",
       "                                         title  upvote_ratio  2_way_label  \\\n",
       "0   Can I still cash in my brain needs to rest           0.9            0   \n",
       "1  How did the wheat say to its son Photoshop?           1.0            0   \n",
       "\n",
       "   comment_sentiment  post_sentiment  \n",
       "0          -0.034000             0.0  \n",
       "1           0.196715             0.0  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Domain Rank Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/cheedcheed/top1m\n",
    "# https://github.com/mozilla/cipherscan/tree/master/top1m\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "class Alexa:\n",
    "    '''\n",
    "    this class provides access to the Alexa ranking of URLs\n",
    "    usage: create a new instance of this class (ranker = Alexa()) and use the get_rank method\n",
    "    '''\n",
    "    __domain_list = []\n",
    "    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            # read the alexa ranking\n",
    "            f_csv = open('top-1m.csv/top-1m.csv')\n",
    "            csv_data = f_csv.read()\n",
    "            f_csv.close()\n",
    "            lines = csv_data.split(\"\\n\")\n",
    "            for line in lines:\n",
    "                try:\n",
    "                    url = line.split(\",\")[1]\n",
    "                    url = re.sub('^www\\.', '', url)\n",
    "                    self.__domain_list.append(url)\n",
    "                except:\n",
    "                    continue\n",
    "        except:\n",
    "            raise\n",
    "        \n",
    "    def get_rank(self, url):\n",
    "        ''' getrank returns the alexa rank of the domain of the given URL, or -1 if it is over 1M'''\n",
    "        parsed_url = urlparse(url)\n",
    "        if parsed_url.scheme == '':\n",
    "            return self.get_rank('http://'+url)\n",
    "        domain = parsed_url.netloc\n",
    "        domain = re.sub('^www\\.', '', domain)\n",
    "        if domain in self.__domain_list:\n",
    "            return self.__domain_list.index(domain)+1   \n",
    "        return 1000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "alexa = Alexa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alexa.get_rank('www.cnn.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_domain_rank(df):\n",
    "    alexa = Alexa()\n",
    "    df['domain_rank'] = df.apply(lambda x: alexa.get_rank(x['domain']), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Domain Rank Column\n"
     ]
    }
   ],
   "source": [
    "post_df = add_domain_rank(post_df)\n",
    "print(\"Added Domain Rank Column\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample Post DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_title</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>domain</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>2_way_label</th>\n",
       "      <th>comment_sentiment</th>\n",
       "      <th>post_sentiment</th>\n",
       "      <th>domain_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>can i still cash in my brain needs to rest</td>\n",
       "      <td>2017-06-17 12:58:22</td>\n",
       "      <td>self.SubredditSimulator</td>\n",
       "      <td>6hrnqx</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>subredditsimulator</td>\n",
       "      <td>Can I still cash in my brain needs to rest</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.034000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>how did the wheat say to its son photoshop</td>\n",
       "      <td>2019-07-27 18:58:22</td>\n",
       "      <td>self.SubredditSimulator</td>\n",
       "      <td>cig6ey</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>subredditsimulator</td>\n",
       "      <td>How did the wheat say to its son Photoshop?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.196715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  clean_title          created_utc  \\\n",
       "0  can i still cash in my brain needs to rest  2017-06-17 12:58:22   \n",
       "1  how did the wheat say to its son photoshop  2019-07-27 18:58:22   \n",
       "\n",
       "                    domain      id  num_comments  score           subreddit  \\\n",
       "0  self.SubredditSimulator  6hrnqx            20     17  subredditsimulator   \n",
       "1  self.SubredditSimulator  cig6ey            20      3  subredditsimulator   \n",
       "\n",
       "                                         title  upvote_ratio  2_way_label  \\\n",
       "0   Can I still cash in my brain needs to rest           0.9            0   \n",
       "1  How did the wheat say to its son Photoshop?           1.0            0   \n",
       "\n",
       "   comment_sentiment  post_sentiment  domain_rank  \n",
       "0          -0.034000             0.0      1000001  \n",
       "1           0.196715             0.0      1000001  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save Final Dataset\n",
    "post_df.to_csv('dataset.csv', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Domain Rank\n",
    "# def get_alexa_ranking(url):\n",
    "#     \"\"\"\n",
    "#     Get Alexa ranking\n",
    "    \n",
    "#     \"\"\"\n",
    "#     from bs4 import BeautifulSoup\n",
    "#     import urllib.request\n",
    "# #     url='9news.com.au'\n",
    "#     rank_str =BeautifulSoup(urllib.request.urlopen(\"https://www.alexa.com/minisiteinfo/\" +url),'html.parser').table.a.get_text()\n",
    "#     try:    \n",
    "#         rank_int=int(rank_str.replace(',',''))\n",
    "#     except:\n",
    "#         rank_int = 1000001\n",
    "#     return rank_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from multiprocess import Pool, Manager\n",
    "# import sentiment\n",
    "# if __name__ == '__main__':\n",
    "#     print(\"Feature Extraction\")\n",
    "#     num_processors = 6\n",
    "#     pool = Pool(processes = num_processors)\n",
    "    \n",
    "#     manager = Manager()\n",
    "#     mgr_score = manager.dict()\n",
    "#     mgr_score.update(comment_scores)\n",
    "#     df_split = np.array_split(comment_df, num_processors)\n",
    "#     for data in df_split:\n",
    "#         pool.apply_async(sentiment.build_comment_dictionary, args = (data, mgr_score, ))\n",
    "#         print(\"done\")\n",
    "#     pool.close()\n",
    "#     pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
